---
layout: distill
title: "Lecture 19"
description: "Sequence Learning with RRNs"
lecturers:
  - "Ben Lengerich"
authors:
  - "Erica Henderson"
editors:
  -
date: 2025-11-10
permalink: /notes/lecture-19/
bibliography: /assets/bibliography/2025-11-10-lecture-18.bib
---

## Overview

Recurrent Neural Networks (RNNs) help model sequential data. Unlike feed forward networks, RNNs maintain a hidden state that captures information from previous time steps, which lets them process variable-length sequences. Long Short-Term Memory (LSTM) networks are introduced as a solution to training challenges seen with RRNs.

---

## Motivation: The Problem with Modeling Text

Traditional machine learning approaches struggle with text data due to several challenges:

- **Variable length input:** Sequences like "Hi" and "This is a very long sentence" have different lengths
- **Long-range dependencies:** Understanding "The movie was great, but..." requires remembering "movie" many words later
- **Need for generative models:** We want to produce text, not just classify it
- **Scale requirements:** LLMs need massive amounts of data

### Historical Approaches

#### Bag-of-Words

Counts word occurrences while ignoring order. A sentence becomes a vector where each dimension represents the count of a specific word in the vocabulary.

**Problem:** "The movie my friend has NOT seen is good" and "The movie my friend has seen is NOT good" produce identical representations despite opposite meanings.

#### Hidden Markov Models

Model sequences where the current state depends only on the previous state, using the Markov assumption: $P(Y_n = y \mid X_1 = x_1, \ldots, X_n = x_n) = P(Y_n = y \mid X_n = x_n)$.

**Problem:** Only considers immediate previous context, cannot capture long-range dependencies.

#### Convolutional Neural Networks

Apply convolutional filters to text sequences.

**Problem:** Cannot naturally handle variable-length input without padding to a maximum length, which can be wasteful and requires an arbitrary maximum.

---

## Recurrent Neural Networks: Core Architecture

In sequential data, **order matters**.

### RNN Equation

The fundamental RNN equation processes sequences while maintaining memory: $h^{(t)} = \sigma(W_{hx} \cdot x^{(t)} + W_{hh} \cdot h^{(t-1)} + b_h)$.

Where:

- $h^{(t)}$: Hidden state at time $t$ (the "memory")
- $x^{(t)}$: Input at time $t$ (current word)
- $W_{hx}$: Weights connecting input to hidden layer
- $W_{hh}$: Weights connecting previous hidden state to current (the recurrent connection)
- $\sigma$: Activation function (typically tanh)

<figure>
  <img src="{{ '/assets/img/notes/lecture-19/singlelayerrrn2.jpeg' | relative_url }}" alt="Single Layer RRN" style="width:80%; display:block; margin:auto;" />
  <figcaption><strong>Figure 1.</strong> The left side looks like the networks we used previously (feedforward) neural networks. Now we unfold a "stacked" version of this to visualize the RRN, where each hidden unit receives 2 inputs.</figcaption>
</figure>

**Key insight:** The same weights $(W_{hx}, W_{hh})$ are used at every time step. This is called **parameter sharing** and enables processing sequences of any length.

### Unfolding Through Time

When "unfolded," an RNN resembles a very deep network where:

- Each time step corresponds to a "layer"
- Information flows both forward (through time) and upward (through the network)
- The depth equals the sequence length

<figure>
  <img src="{{ '/assets/img/notes/lecture-19/multilayerrrn.jpeg' | relative_url }}" alt="Multi Layer RRN" style="width:80%; display:block; margin:auto;" />
  <figcaption><strong>Figure 1.</strong> A multilayer RRN has more than one hidden unit.</figcaption>
</figure>

---

## Different RNN Architectures

### Many-to-One

- **Input:** Sequence (e.g., movie review text)
- **Output:** Single value (e.g., positive/negative sentiment)
- **Use cases:** Sentiment analysis, text classification

### One-to-Many

- **Input:** Single value (e.g., image)
- **Output:** Sequence (e.g., caption)
- **Use cases:** Image captioning, music generation from genre

### Many-to-Many

- **Input:** Sequence
- **Output:** Sequence

Two variants:

1. **Direct:** Output at each time step (video captioning frame-by-frame)
2. **Delayed:** Read entire input, then generate output (machine translation)

<figure>
  <img src="{{ '/assets/img/notes/lecture-19/weighmatrixrrn.jpeg' | relative_url }}" alt="Weight Matrix of RRN" style="width:80%; display:block; margin:auto;" />
  <figcaption><strong>Figure 1.</strong> This image visualizes the inputs and outputs of RRNs.</figcaption>
</figure>

---

## Training RNNs: Backpropagation Through Time

### The Chain Rule Problem

Training RNNs requires computing gradients through time using the chain rule:

$\frac{\partial L^{(t)}}{\partial W_{hh}} = \frac{\partial L^{(t)}}{\partial y^{(t)}} \cdot \frac{\partial y^{(t)}}{\partial h^{(t)}} \cdot \left(\sum_{k=1}^t \frac{\partial h^{(t)}}{\partial h^{(k)}} \cdot \frac{\partial h^{(k)}}{\partial W_{hh}}\right)$

The critical component is: $\frac{\partial h^{(t)}}{\partial h^{(k)}} = \prod_{i=k+1}^t \frac{\partial h^{(i)}}{\partial h^{(i-1)}}$, which is a **product of many terms** that leads to fundamental problems.

### Vanishing and Exploding Gradients

Since we're multiplying many terms in the gradient computation:

- **If each term < 1:** Product approaches 0 (vanishing gradients)
- **If each term > 1:** Product explodes (exploding gradients)

**Why this matters:**

- **Vanishing gradients:** Cannot learn long-range dependencies (can't connect "movie" 50 words ago to current prediction)
- **Exploding gradients:** Training becomes unstable with NaN or Inf values

### Solutions

#### 1. Gradient Clipping (for Exploding Gradients)

If the gradient norm exceeds a threshold, scale it down proportionally. This caps the gradient magnitude while preserving direction.

#### 2. Truncated BPTT

Instead of backpropagating through 100 time steps for example, only go back 20.

- **Pro:** More stable training
- **Con:** Cannot learn very long dependencies

#### 3. LSTM Architecture

The fundamental solution that enables learning long-range dependencies.

---

## Long Short-Term Memory (LSTM)

LSTMs add a **separate memory pathway** (the cell state $C$) that runs parallel to the hidden state ($h$), providing a "gradient highway" for information flow.

### Cell State

The cell state $C^{(t-1)} \rightarrow C^{(t)}$ is the "memory highway" where information can flow largely unchanged, solving the vanishing gradient problem by allowing gradients to flow back without being multiplied many times.

### The Three Gates

#### Forget Gate

$f_t = \sigma(W_{fx} \cdot x^{(t)} + W_{fh} \cdot h^{(t-1)} + b_f)$

**Purpose:** Decides what information to discard from the cell state.

The sigmoid function $\sigma$ outputs values between 0 and 1:

- 0 = "completely forget this"
- 1 = "completely keep this"

**Example:** When reading "The cat, which was very fluffy, ..." the LSTM might forget details about the cat once we move to a new sentence.

#### Input Gate

$i_t = \sigma(W_{ix} \cdot x^{(t)} + W_{ih} \cdot h^{(t-1)} + b_i)$

$g_t = \tanh(W_{gx} \cdot x^{(t)} + W_{gh} \cdot h^{(t-1)} + b_g)$

**Purpose:** Decides which new information to store in the cell state.

- $i_t$: Determines which new information to store
- $g_t$: Proposes candidate values to add
- Together: $i_t \odot g_t$ (element-wise multiplication)

**Example:** When encountering "The dog" in a new sentence, the input gate decides to store "subject is now dog, not cat".

#### Output Gate

$o_t = \sigma(W_{ox} \cdot x^{(t)} + W_{oh} \cdot h^{(t-1)} + b_o)$

$h^{(t)} = o_t \odot \tanh(C^{(t)})$

**Purpose:** Decides what parts of the cell state to output as the hidden state.

- $\tanh(C^{(t)})$: Squashes cell state to [-1, 1]
- $o_t$: Filters what to actually output

**Example:** The LSTM might remember many details about the story but only output information relevant to the current prediction task.

### Cell State Update

$C^{(t)} = C^{(t-1)} \odot f_t + i_t \odot g_t$

- **First term:** What we keep from old memory (forget gate filters old memory)
- **Second term:** What we add from new input (input gate filters new information)

### Complete LSTM Forward Pass

1. **Forget:** Decide what to forget from $C^{(t-1)}$
2. **Input:** Decide what new information to add
3. **Update:** Actually update cell state
4. **Output:** Decide what to output from cell state

### Why LSTMs Work

- The cell state provides a **gradient highway** where gradients can flow back through time more easily
- Gates learn to dynamically preserve important information and forget irrelevant details
- The network decides adaptively what to remember and forget based on the current context

---

## Preparing Text for RNNs

### Step 1: Build Vocabulary

Create a mapping from words to integer indices, including special tokens:

- Unknown words not in vocabulary
- Padding for different length sequences
- Start of sequence
- End of sequence

### Step 2: Convert Text to Indices

Transform text sequences into sequences of vocabulary indices. For example:

"The sun is shining" → [7, 5, 2, 4, ..., 10, 10, 10]

Sequences are typically padded to a common length for batch processing.

### Step 3: Embeddings (Not One-Hot Encoding)

**Old approach (One-Hot):** Represent word at index 7 as a sparse vector with 1 at position 7 and 0s elsewhere.

**Problem:** Very sparse! For a 10,000-word vocabulary, each word becomes a 10,000-dimensional vector with all zeros except one position.

**Modern approach (Embeddings):** Each word is represented by a dense, learned vector of lower dimension (e.g., 300-D instead of 10,000-D).

An embedding matrix maps vocabulary indices to dense vectors: $\text{embedding} \in \mathbb{R}^{|V| \times d}$, where $|V|$ is vocabulary size and $d$ is embedding dimension.

**Key properties of learned embeddings:**

- Much smaller dimension (e.g., 300 vs 10,000)
- Similar words have similar embeddings (learned automatically!)
- Semantic relationships emerge (famous example: "king" - "man" + "woman" ≈ "queen")

PyTorch's `nn.Embedding` layer implements this as a learnable lookup table.

---

## Connecting to Generative Models

RNNs model the **auto-regressive probability distribution**: $P_\theta(X) = \prod_t P_\theta(X_t \mid X_{<t})$.

This is exactly what language models do:

- Predict the next token given all previous tokens
- The training objective maximizes log-likelihood of observed sequences: $\max_\theta \sum_{i} \sum_{t} \log P_\theta(X_{i,t} \mid X_{i,<t})$

RNNs were the first architecture that could effectively model this sequential dependency structure at scale.

---

## Key Takeaways

1. **Sequential Processing:** RNNs process sequences by maintaining a hidden state that captures information from previous time steps through the recurrent connection $h^{(t-1)} \rightarrow h^{(t)}$.

2. **Gradient Problems:** Vanishing and exploding gradients are fundamental challenges when training RNNs on long sequences, arising from the multiplicative nature of backpropagation through time.

3. **LSTM Solution:** LSTMs address gradient problems through:

   - Separate cell state (memory highway)
   - Three gates (forget, input, output) that learn what to remember and forget
   - Ability to learn dependencies across hundreds of time steps

4. **Text Preprocessing:** Modern text processing uses learned embeddings (not one-hot encoding) to create dense, semantic representations where similar words have similar vectors.

5. **Architectural Flexibility:** RNNs can be configured for different tasks:

   - Many-to-one: Classification
   - One-to-many: Generation from seed
   - Many-to-many: Translation

6. **Historical Context:** While transformers have largely replaced RNNs for NLP tasks, understanding RNNs is crucial for:
   - Sequential modeling fundamentals
   - Understanding why attention mechanisms were needed
   - Current time-series and signal processing applications

---
